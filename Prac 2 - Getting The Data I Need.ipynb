{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prac 2 - Getting the Data I need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Complete the tasks given. \n",
    "* <strong>Please note that your code will be re-executed on new dataset files during marking. The new dataset will have the same \"Column Name\" as that of the current dataset - but the order of the columns can be different. Also the rows, and number of rows will change. Your code should give the correct answer for the new datasets as well. <u>So please don't hard-code the answers / row indexes / column indexes.</u> To reference a column you can use the column names instead of column index.</strong>\n",
    "\n",
    "* <strong>Make sure the R script submitted has no syntactical error, in which case a zero will be awarded for this Prac. Tutors will direct you on how to identify syntactical errors in your script.</strong>\n",
    "\n",
    "Additional Note\n",
    "* Variable names and strings are case-sensitive in R\n",
    "* Use of any packages to achieve the objective is strictly prohibited - unless explicitly mentioned in the question to do so\n",
    "* Any updates regarding the Pracs will be posted on <strong>Piazza</strong>\n",
    "* Your code will be tested on Version 4.x.x of R\n",
    "* You may find it helpful to read through the whole notebook and learn from the examples first, before solving the problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "After completing the tasks, download the notebook as an R file to your local system. This can be done by \"File > Download as > R\". Rename the downloaded R file with your \"UQ Login-id\" - for example, `s4477608.r`\n",
    "* Upload the downloaded file to jupyter in the same folder as that of this notebook, that is, inside \"Prac2\" folder.\n",
    "* Submit the downloaded file to BB.\n",
    "\n",
    "<strong> The R file uploaded to jupyter will be assessed automatically. So, it is very important to upload the R file to the correct location in jupyter, with the correct name ( as mentioned above ) - failing to which 0 will be awarded for this Prac. </strong>\n",
    "\n",
    "Make sure you \"Save & Checkpoint\" this jupyter notebook as well before the deadline. DO NOT rename this jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Assessing the Ethical Use of Data\n",
    "\n",
    "An important ethical consideration for releasing data is making sure that it is properly de-anonymised. In this part of the pratical, we will demonstrate how a dataset that at first seems  anonymised may potentially be de-anonymised. We will also walk through some potential steps to mitigate this risk.\n",
    "\n",
    "Let's say we have been given access to an *anonymised* dataset containing UQ student GPA information. Someone has attempted to anonymise this dataset by removing student names and IDs from the data. Download the dataset onto your lab machine:\n",
    "\n",
    "[student_data_anon.csv](./student_data_anon.csv)\n",
    "\n",
    "Let's also say we've managed to access a separate dataset that contains names, ages and postcodes from a UQ sporting club. Download the dataset onto your lab machine:\n",
    "\n",
    "[club_data.csv](./club_data.csv)\n",
    "\n",
    "Using the club dataset, can we possibly de-anonymise some of the students contained within the GPA dataset? Have a look at both datasets and their headers in Excel, can you think of any ways we can combine the two datasets to determine the names of the students in the anonymised student data?\n",
    "\n",
    "### Databases\n",
    "\n",
    "In the first prac we imported data from a single flat file using R. Data is commonly spread across multiple data sets or files, and often more insightful information can be gained by combining several datasets.\n",
    "\n",
    "One solution for exploring multiple datasets is to import the datasets into a *database*. In this practical, we will look at the relational database `MySQL`, which uses a standard format to interact, merge and answer questions with data called SQL (Structured Query Language).\n",
    "\n",
    "We will demonstrate the use of exploring multiple datasets by attempting to *de-anonymise* data after importing it into `MySQL`.\n",
    "\n",
    "### Importing into Phpmyadmin\n",
    "phpMyAdmin is a web tool designed to handle administrating MySQL databases. Your zone already has MySQL and phpMyAdmin configured, you can find it at `https://data7001-XXXXXXX.uqcloud.net/phpmyadmin/` (replacing XXXXXXX with your zone id. For example, if https://data7001-06431cf7.uqcloud.net/ is you jupyter zone link then \"06431cf7\" is your zone id. So link for phpMyAdmin would be https://data7001-06431cf7.uqcloud.net/phpmyadmin).\n",
    "\n",
    "The first thing we need to do is to import our CSV files into phpMyAdmin. If the CSV is in the correct format, MySQL will parse the CSV file, create the associated `table` and import the data into the table as rows. A MySQL table consists of rows and columns. Columns specify the *type* of data (e.g. whether it is text, a number, a 'True' or 'False' value etc) and the rows contain the actual data itself. Below is a visual representation of a MySQL table, as displayed by phpMyAdmin:\n",
    "\n",
    "![](img/mysql-table-in-phpmyadmin.png)\n",
    "\n",
    "Let's import the `student_data_anon.csv` file first. On the top menu of phpMyAdmin, select the `Import` option. Under the **File To Import** header, click the 'Choose File' button and browse to the location of `student_data_anon.csv` on the local system. Under the **Format** header, select `CSV` in the drop down menu, and enable the option \"The first line of the file contains the table column names\". Then press the \"Go\" button.\n",
    "\n",
    "This will create a database called \"CSV_DB\" and a table called \"TBL_NAME\" (both generic filler names) which is then filled in with the values from our CSV file. Let's rename the table name to something more meaningful - click on the `TBL_NAME` table on the left under `CSV_DB`. In this menu, click the **Operations** tab in the top menu and rename the database to `student_data_anon` under **Table Options** and press the 'Go' button.\n",
    "\n",
    "Let's import our second dataset now - `club_data.csv`. First click `CSV_DB` on the left hand menu so we are operating within the same database as our first dataset. Then press the \"Import\" tab on the top menu and follow the same steps previously outlined to import our second dataset. Rename the new table to `club_data`.\n",
    "\n",
    "### Exploring The Data\n",
    "\n",
    "Let's now have a look at how we can use SQL to ask some interesting questions about the data. Click on CSV_DB again in the left hand column and then click the 'SQL' tab on the top menu.\n",
    "\n",
    "This menu allows us to query the database using the SQL language. \n",
    "\n",
    "For example, we might ask something like - how many females are in our dataset? Try pasting in the following SQL query into the bottom input box under **SQL query on database CSV_DB:** and then press `Submit Query`.\n",
    "\n",
    "```\n",
    "SELECT COUNT( * ) \n",
    "FROM student_data_anon\n",
    "WHERE gender =  'F'\n",
    "```\n",
    "\n",
    "We can add as many filters as we want to start asking more specific questions - how many females in our dataset had a GPA less than 4?\n",
    "\n",
    "```\n",
    "SELECT COUNT( * ) \n",
    "FROM student_data_anon\n",
    "WHERE gender =  'F'\n",
    "AND GPA < 4.0\n",
    "```\n",
    "\n",
    "You can see from the above example queries that SQL statements follow a general format. We have a `SELECT <x>` query that specifies what specific results we want - in this case, the `COUNT(*)` function returns the number of rows that matches a specified criteria. We then have a `FROM` statement which specifies which **table** we want to pull our results from. We could easily run the first query against the `club_data` table (how many women are in the club_data dataset?) with the following syntax:\n",
    "\n",
    "```\n",
    "SELECT COUNT( * )\n",
    "FROM club_data\n",
    "WHERE gender = 'F'\n",
    "```\n",
    "\n",
    "The last `WHERE` clause specifies our filter, and the `AND` statement can be used to add more filters.\n",
    "\n",
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| Modify and execute the above query to determine how many **men** had a GPA **greater than** 4.0 in our `student_data_anon` table? Provide SQL query AND the final answer. <br> Store the SQL Query as text inside \"menGt4_SQL\" R variable, and the final answer inside \"menGt4\" R variable. <br>You can hard-code the final answer inside R variables.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menGt4_SQL = \"SELECT COUNT(*) FROM student_data_anon WHERE gender = 'M' AND GPA > 4.0\"\n",
    "menGt4 = 267"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure your include your code for all questions requiring coding.**\n",
    "\n",
    "SQL supports many more functions. A tutorial on many of them are available at https://www.w3schools.com/sql/. The following task requires using the `AVG` and `MAX` functions.\n",
    "\n",
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| (a) what is the average GPA of the students in the`student_data_anon` table? Store the SQL Query as text inside \"avgGpa1_SQL\" R variable, and the final answer inside \"avgGpa1\".  <br>(b) What is the maximum GPA? Store the SQL Query as text inside \"maxGpa1_SQL\" R variable, and the final answer inside \"maxGpa1\" R variable. <br> You can hard-code the final answer inside R variables.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "avgGpa1_SQL = \"SELECT AVG(GPA) FROM student_data_anon\"\n",
    "avgGpa1 = 4.5533\n",
    "\n",
    "maxGpa1_SQL = \"SELECT MAX(GPA) FROM student_data_anon\"\n",
    "maxGpa1 = 6.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are often interested in the groups within a collection of data, and the `group by` statement in SQL is useful for this. See https://www.w3schools.com/sql/ for an example. There are also some examples of using `group by` below, and you want may to answer the task below after you study them.\n",
    "\n",
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| Write a SQL query to generate a table of all unique postcodes and the corresponding average GPAs for students living in areas with the same postcode, with the postcodes ordered by increasing average GPAs. Store the SQL query inside \"incAvgGpa_SQL\" R variable. <br> Store the postcode which has the highest average GPA inside \"highestAvg1\" R variable. <br> You can hard-code the final answer inside R variables. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "\n",
    "incAvgGpa_SQL = \"SELECT postcode, AVG(GPA) as AGPA FROM student_data_anon GROUP BY postcode ORDER BY AGPA\"\n",
    "highestAvg1 = 4069"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Tables\n",
    "So we now know how to do some very basic queries across a single table. However, we want to be able to use *both tables* to see if we can use information from the club data to identify some of the users in the student dataset. This `club_data` dataset will be our *adversarial* dataset.\n",
    "\n",
    "As gender, age and postcode are available in both datasets, these three fields will be our *quasi-identifiers*. Quasi-identifiers are pieces of information that, alone, are not able to uniquely identifiy a record. However, they are correlated enough to potentially create a unique identifier.\n",
    "\n",
    "In our student data example, trying to identify a student based on gender, age or postcode alone would be difficult. However, the combination of all three may be sufficient to identify individuals.\n",
    "\n",
    "Our goal is to use our `club_data` dataset which contains these quasi identifiers to deanonymise this data. We can achieve this by using an *inner join* in MySQL. An inner join allows us to match rows in one table with rows in another table only if both tables meet the conditions specified.\n",
    "\n",
    "For example, let's say we have only 2 entries in club_data that looks like the following:\n",
    "\n",
    "| gender | age | postcode | firstname | surname  |\n",
    "|--------|-----|----------|-----------|----------|\n",
    "| F      | 23  | 4068     | Jane      | Anderson |\n",
    "| M      | 20  | 4044     | Thomas    | Hill     |\n",
    "\n",
    "We also have 3 entries in student_data_anon that looks like the following:\n",
    "\n",
    "| gender | age | postcode | GPA |\n",
    "|--------|-----|----------|-----|\n",
    "| M      | 31  | 4011     | 5.3 |\n",
    "| F      | 23  | 4068     | 5.5 |\n",
    "| F      | 20  | 4000     | 4.0 |\n",
    "\n",
    "Is there a combination of gender, age and postcode that appear in both tables?\n",
    "\n",
    "If we do an *inner-join* on the tables on these elements, we can combine the tables on these matching entries, and get a result that looks like the following:\n",
    "\n",
    "| gender | age | postcode | firstname | surname  | GPA |\n",
    "|--------|-----|----------|-----------|----------| --- |\n",
    "| F      | 23  | 4068     | Jane      | Anderson | 5.5 |\n",
    "\n",
    "You can see from this example that an *inner-join* is the act of selecting *matching entries* for specific *identifiers* from two different tables.\n",
    "\n",
    "![](img/inner-join.png)\n",
    "\n",
    "Let's do the same thing to our actual datasets and see if we can potentially match individuals to their GPA. Open the SQL editor again, and submit the following query:\n",
    "\n",
    "```\n",
    "SELECT c.* , s.GPA\n",
    "FROM club_data c\n",
    "INNER JOIN student_data_anon s ON ( s.gender = c.gender) AND (s.age = c.age) AND (s.postcode=c.postcode)\n",
    "```\n",
    "\n",
    "|<center>QUESTION</center>|\n",
    "| ---- |\n",
    "| Are there any *gender,age,postcode* combinations that have more than one  matching individual? If yes, how many such combinations are there? <br> Store the SQL query and final answer for the count of combinations inside \"gap1_SQL\" and \"gap1\" R variables respectively. <br> You can hard-code the final answer inside R variables.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "gap1_SQL = \"SELECT COUNT(*) FROM( SELECT COUNT(*) AS count FROM club_data AS cd INNER JOIN student_data_anon AS sd ON cd.gender=sd.gender AND cd.age=sd.age AND cd.postcode=sd.postcode GROUP BY sd.gender, sd.age, sd.postcode HAVING count(*) > 1) AS A\"\n",
    "gap1 = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>QUESTION</center>|\n",
    "| ---- |\n",
    "| What is Debra Gibson' GPA? <br> Store the SQL Query and final answer inside \"debra1_SQL\", and \"debra1\" R variables respectively. <br> You can hard-code the final answer inside R variables. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "debra1_SQL = 'SELECT sd.GPA FROM club_data AS cd INNER JOIN student_data_anon AS sd ON cd.gender=sd.gender AND cd.age=sd.age AND cd.postcode=sd.postcode WHERE cd.first_name=\"Debra\" AND cd.last_name=\"Gibson\"'\n",
    "debra1 = 5.87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Anonymity\n",
    "The above examples demonstrate that student_data_anon, despite having the names removed, is still vulnerable to de-anonymisation with an adversarial dataset. One potential method of better anonymising the data is to make sure it satisfies the *k-anonymity* property for our quasi-identifiers of gender, age and postcode.\n",
    "\n",
    "So far we have only *read* data from MySQL, but for this part of the prac we want to modify the data in the database using the `UPDATE` command.\n",
    "\n",
    "Let's first copy our original database so we do not lose any accidental data! First click on the database name on the left column, `CSV_DB`. Then, one the top menu, click the 'Operations' tab. Under the 'Copy Database To' heading, give your database copy the name `CSV_DB_CP`, and make sure \"Structure and data\" and \"CREATE DATABASE before copying\" and \"Add AUTO_INCREMENT value\" is selected.\n",
    "\n",
    "We will now perform all our queries on our new copy of the database, click the 'CSV_DB_CP' database on the left and open the Query menu again.\n",
    "\n",
    "Recall from your lectures that k-anonymity property is satisfied for each individual if for each unique combination of quasi-identifiers, there are *at least k-1 individuals whose information also appear in the dataset*.\n",
    "\n",
    "We can use the following SQL statement to demonstrate how many individual rows match certain combinations of gender, age and postcode:\n",
    "\n",
    "```\n",
    "SELECT gender, age, postcode, COUNT( * ) AS counts\n",
    "FROM student_data_anon\n",
    "GROUP BY gender, age, postcode\n",
    "```\n",
    "\n",
    "Can we modify our anonymised student data such that there are at least two rows in every unique combination of gender, age and postcode (2-anonymity)?\n",
    "\n",
    "One solution may be to rounding the ages to the nearest decade. So a student who is 17 gets changed to 20, and a student who is 34 gets changed to 30.\n",
    "\n",
    "We can use the MySQL \"round()\" function to round values.\n",
    "\n",
    "Enter the following command to update the ages in the `student_data_anon` table to the nearest decade.\n",
    "\n",
    "```\n",
    "UPDATE student_data_anon SET age = ROUND(age, -1)\n",
    "```\n",
    "\n",
    "Make sure it worked by examining the table:\n",
    "\n",
    "```\n",
    "SELECT * \n",
    "FROM student_data_anon\n",
    "```\n",
    "\n",
    "Do we have less unique combinations now? Let's try our original combination count query but limit results to only those that have a count of \"1\":\n",
    "\n",
    "```\n",
    "SELECT gender, age, postcode, COUNT( * ) AS counts\n",
    "FROM student_data_anon\n",
    "GROUP BY gender, age, postcode\n",
    "HAVING COUNT( * ) = 1\n",
    "```\n",
    "\n",
    "Unfortunately it looks like there are still unique combinations. What if we round postcodes to the nearest decade, too?\n",
    "\n",
    "```\n",
    "UPDATE student_data_anon SET postcode = ROUND(postcode, -1)\n",
    "```\n",
    "\n",
    "It looks like we still have a few issues - many students over 40 are still uniquely identifiable, and it looks like students that had data entry issues without a gender specified are also still unique.\n",
    "\n",
    "One potential solution might be to remove any entries that have poor data, so our dataset only includes full entries. Lets submit another query where all entries that have an empty 'gender' column are removed:\n",
    "\n",
    "```\n",
    "DELETE FROM student_data_anon WHERE gender =  ''\n",
    "```\n",
    "\n",
    "Let's also update ages so that any entry with an age greater than 50 is set to 50:\n",
    "\n",
    "```\n",
    "UPDATE student_data_anon SET age = 50 WHERE age > 50\n",
    "```\n",
    "\n",
    "Let's have a look again to see if we still have any unique combinations:\n",
    "```\n",
    "SELECT gender, age, postcode, COUNT( * ) AS counts\n",
    "FROM student_data_anon\n",
    "GROUP BY gender, age, postcode\n",
    "HAVING COUNT( * ) = 1\n",
    "```\n",
    "\n",
    "It looks like we have successfully removed all unique identifying combinations of our *quasi-identifiers*, gender, age and postcode!\n",
    "\n",
    "We can use the following query to find the new minimum frequency of each combination of gender, age and postcode:\n",
    "\n",
    "```\n",
    "SELECT MIN( mycount ) \n",
    "FROM (\n",
    "SELECT COUNT( * ) mycount\n",
    "FROM student_data_anon\n",
    "GROUP BY gender, age, postcode\n",
    ") AS counts\n",
    "```\n",
    "\n",
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| What level of k-anonymity is our dataset now? Store the answer inside \"k1\" variable. <br> You can hard-code the final answer inside R variables. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "k1=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2 - Hadoop Distributed Filesystem (HDFS)\n",
    "In the previous exercise we demonstrated ingesting data from a flat CSV file into a *database*, where we used SQL to explore the data. While databases like MySQL are effective at analysing small datasets, we begin to run into complex scalability issues when the size of the dataset exceeds what can be stored on a single machine. We can continue to add more disk space as the dataset grows, but then we begin to run into problems with processing speed - a single CPU can only process so much data off disk at a time.\n",
    "\n",
    "A potential solution is to scale the work *and the data* over *multiple machines*, so that processing occurs in parallel across several machines and several different hard disks. Scaling up this sort of infrastructure would just involve adding new machines, and is often referred to as *horizontal scaling*.\n",
    "\n",
    "One solution that uses horizontal scaling for storing large datasets for processing is the *Hadoop Distributed Filesystem (HDFS)*. A distributed file system is simply a file system where the data is stored on a server, not the local client machine. In HDFS, *where* the data is stored is abstracted away from the user. You can browse the data as if you were browsing on a local machine, however the dataset could potentially be stored across many different computers.\n",
    "\n",
    "In this part of the prac, we will gain some familiarity with the HDFS command line tools, and ingest some data into HDFS to be explored in future pracs.\n",
    "\n",
    "### Connect to the remote DATA7001 node\n",
    "Open the ‘Terminal’ program on the lab PC. Use SSH to connect to the DATA7001 Client Node, entering your UQ student password at the prompt.\n",
    "\n",
    "```\n",
    "ssh <username>@clientnode.zones.eait.uq.edu.au\n",
    "```\n",
    "\n",
    "NOTE: Your password won't appear on the screen as you type it in, this is to be expected – just type your password and hit the `return` key. If you are not inside a UQ network, you will need to first run `ssh <username>@remote.labs.eait.uq.edu.au`, and then run `ssh <username>@clientnode.zones.eait.uq.edu.au`.\n",
    "\n",
    "You should be presented with a screen that looks like this:\n",
    "\n",
    "```\n",
    "     ____________\n",
    "     |   \\XX/   |\n",
    "     | T. \\/ .T |      University of Queensland\n",
    "     | XX:  :XX |          Faculty of EAIT\n",
    "     T L' /\\ 'J T\n",
    "      \\  /XX\\  /\n",
    "   @\\_ '______' _/@\n",
    "   \\_X\\_ ____ _/X_/\n",
    "     \\=/\\----/\\=/\n",
    "\n",
    "-----------------------------------------------\n",
    "              DATA7001 Client Node\n",
    "-----------------------------------------------\n",
    "[sXXXXXX@data7001 ~]$\n",
    "```\n",
    "\n",
    "### Download the HR Analytics Dataset\n",
    "\n",
    "You can download the HR Analytics dataset onto the client node using the wget program. Like other unix programs, you can use the manual page to learn about how to use it. Try using the command `man wget` to read the manual for the wget program. The typing the letter `q` will exit the man page.\n",
    "\n",
    "Run the command \n",
    "\n",
    "```\n",
    "wget https://stluc.manta.uqcloud.net/mdatascience/public/datasets/HumanResourceAnalytics/HR_comma_sep.csv\n",
    "```\n",
    "\n",
    "to download the HR analytics dataset onto the data7001 node.\n",
    "\n",
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "|  Use `wget` to download the HR Analytics Dataset from the link specified at the beginning of the prac. |\n",
    "\n",
    "### Move data between the local filesystem and HDFS\n",
    "So far we have downloaded the HR Analytics Dataset to the local filesystem. To be able to use the Hadoop tools, we first have to push the file from our local filesystem into HDFS.\n",
    "\n",
    "As HDFS is modelled similarly to the Unix filesystem, many of the commands will seem similar to local filesystem commands. We will be using the `hadoop fs` set of commands, an overview can be found on the Hadoop file system shell documentation page:\n",
    "\n",
    "https://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/FileSystemShell.html \n",
    "\n",
    "`$ hadoop fs -ls` for example is analogous to the ls command on the local filesystem.\n",
    "\n",
    "To push a local file into HDFS, we can use:\n",
    "\n",
    "`$ hadoop fs -put [local path] [hdfs path]`\n",
    "\n",
    "For example, I can push a file called “testfile” into the /tmp directory of HDFS with the following command:\n",
    "\n",
    "`$ hadoop fs -put testfile /tmp/testfile`\n",
    "\n",
    "To read a file from HDFS, we first need to copy it onto our local filesystem. To copy a file from HDFS to our local filesystem, we can use the following command:\n",
    "\n",
    "`$ hadoop fs -get [hdfs path] [local path]`\n",
    "\n",
    " For example, I can get the file we just pushed into `/tmp/testfile` on HDFS and rename it `testfile2` on my local system with the following command:\n",
    " \n",
    " `$ hadoop fs -get /tmp/testfile testfile2`\n",
    "\n",
    "To see what the file says, we can use the program cat (a command line program that can (among other things) output the contents of a file onto the terminal:\n",
    "\n",
    "```\n",
    "$ cat testfile\n",
    "hello world!\n",
    "```\n",
    "\n",
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "|  What does the file called `SECRET` within your home directory on HDFS contain? |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "```\n",
    "$ hadoop fs -cat SECRET # Output the SECRET file directly from HDFS onto the terminal screen\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "|  Use the hadoop fs tools to push the HR Analytics dataset you downloaded from the previous question into your home directory in HDFS. Note: Dont change the name of the file |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Solution**\n",
    "\n",
    "`$ hadoop fs -copyFromLocal HR_comma_sep.csv HR_comma_sep.csv`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "|  Use the hadoop fs tools to make a copy of the file SECRET, called SECRET_copy in HDFS. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Solution**\n",
    "\n",
    "`$ hadoop fs -cp SECRET SECRET_copy`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Reasoning with sampling strategies\n",
    "\n",
    "\n",
    "### Simple Random Sampling\n",
    "\n",
    "As we saw in lectures, simple random sampling with replacement (SRSWR) can be accomplised as follows.  \n",
    "\n",
    "Suppose the rows of the n by d array \"data\" correspond to entries from which we wish to resample m times.\n",
    "\n",
    "The syntax of the R command is:\n",
    "\n",
    "`datasample<-data[sample(1:n,m,replace=TRUE),]`\n",
    "\n",
    "Let us try this now with the anonymised student data.  \n",
    "\n",
    "We will now load the data into R and display summary statistics.\n",
    "\n",
    "Note that, for simplicity, we will drop any rows with missing data, and only deal with complete cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- read.csv(\"student_data_anon.csv\", na.strings=c(\"\", \"NA\"))\n",
    "data <- data[complete.cases(data),]\n",
    "data <- as.data.frame(data)\n",
    "summary(data)\n",
    "mean(data$age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will now take a SRSWR of size 100 from the 835 case-complete rows, and compute the mean GPA of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasample<-data[sample(1:nrow(data),100,replace=TRUE),]\n",
    "mean(datasample$GPA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are interested in calculating the mean GPA by gender, as well as the number of observations by gender.  \n",
    "\n",
    "We will now do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate(GPA~gender, datasample, mean)\n",
    "aggregate(GPA~gender, datasample, length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have taken a single SRSWR from our overall dataset and computed the mean GPA by gender.  \n",
    "\n",
    "Repeated resampling is the basis of **bootstrapping**, wherein one constructs a set of statistics of the repeated samples.\n",
    "\n",
    "For more information on Bootstrapping, see pp 189 to 190 of the following text: http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf\n",
    "\n",
    "These then give one an empirical understanding of the distribution of those statistics as they relate to the population of interest.\n",
    "\n",
    "We will now compute and store the mean by gender for a SRSWR of size 100 repeatedly (1000 times), compute summary statistics of the mean by gender, and display a histogram of the mean by gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanGPAbygender<-data.frame(sample=integer(),FGPA=double(),MGPA=double())\n",
    "for (i in 1:1000){\n",
    "    set.seed(i)\n",
    "    datasample<-data[sample(1:nrow(data),100,replace=TRUE),]\n",
    "    temp<-as.data.frame(aggregate(GPA~gender, datasample, mean))\n",
    "    temp$gender<-NULL\n",
    "    meanGPAbygender[i,]<-c(i,t(temp))\n",
    "}\n",
    "\n",
    "summary(meanGPAbygender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us plot the histograms.\n",
    "\n",
    "A histogram is a simple and widely used graphical representation of the distribution of numerical data.  \n",
    "\n",
    "The range of data is partitioned into bins, and the number of data entries in each bin (the frequencies) are counted.  The histogram is then constructed as a collection of rectangles whose base is the bin and whose height is the corresponding bin's frequency.  See also https://en.wikipedia.org/wiki/Histogram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(meanGPAbygender$MGPA, col=rgb(0,0,1,0.5),main=\"Bootstrapped Histogram for Mean GPA by Gender\", xlab=\"Mean GPA\")\n",
    "hist(meanGPAbygender$FGPA, col=rgb(1,0,0,0.5),add=TRUE)\n",
    "legend(\"topright\", c(\"Male\",\"Female\"),fill=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>Task<center>|\n",
    "| ------- |\n",
    "|<center>From the histograms and summary statistics above, what can you say about the comparative *location* or *central tendency* of the mean GPA between the two groups? How about the comparative *spread* or *variability* of the mean GPA? <center>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Solution**\n",
    "\n",
    "The central tendency of the GPA for males is a little bit higher than the tendency for females for this model. The variability (or variance) of both categories (male and female) seems to be pretty close. Both histograms appear to resemble normal distribution (bell-shaped curve).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Random Sampling\n",
    "\n",
    "For SRSWR, each observation is equally likely to appear in the sample.  Sometimes, we wish to select each observation with a probability proportional to a specified weight.  As an illustrative example, we may have demographic data from the ABS which tells us the actual proportions of males and females by age-group enrolled at university, and we wish to correct for *sampling bias*.\n",
    "\n",
    "For the purposes of this practical, we will create a non-negative weight without any special meaning for each of the n rows in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random weights which are non-negative and sum to 1\n",
    "set.seed(55)\n",
    "data$weights<-runif(nrow(data))\n",
    "data$weights<-data$weights/sum(data$weights)\n",
    "\n",
    "weighteddatasample<-data[sample(1:nrow(data),100,replace=TRUE,data$weights),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>Task</center>|\n",
    "| ---- |\n",
    "| Using this sampling approach, repeat the steps we carried out for SRSWR to create a Weighted Sample Bootstrapped Histogram for Mean GPA by Gender. <br> Store the mean values for male and female inside \"maleWeighted1\" and \"femaleWeighted1\" variables respectively.  <br> Set the seed to iteration number itself for bootstrapped sampling. <br> Remember: the iteration numbers are from 1 to 1000. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "\n",
    "meanGPAbygenderWeighted<-data.frame(sample=integer(), MGPA=double(), FGPA=double())\n",
    "for (i in 1:1000){\n",
    "    set.seed(i)\n",
    "    datasample <- data [ sample(1:nrow(data), 100, replace=TRUE, data$weights) , ]\n",
    "    temp <- as.data.frame( aggregate(GPA~gender, datasample, mean) )\n",
    "    temp$gender<-NULL\n",
    "    meanGPAbygenderWeighted[i,] <- c(i,t(temp))\n",
    "}\n",
    "\n",
    "maleWeighted1 = meanGPAbygenderWeighted$MGPA\n",
    "femaleWeighted1 = meanGPAbygenderWeighted$FGPA\n",
    "\n",
    "hist(maleWeighted1, col=rgb(0,0,1,0.5),main=\"Bootstrapped Histogram for Mean GPA by Gender, Weighted\", xlab=\"Mean GPA\")\n",
    "hist(femaleWeighted1, col=rgb(1,0,0,0.5),add=TRUE)\n",
    "legend(\"topright\", c(\"Male\",\"Female\"),fill=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Sampling\n",
    "\n",
    "For Stratifed Sampling on s strata, we generate a SRSWR of size k for each and then combine to obtain a sample of size m=sk.  In the combined sample, each of the s strata has exactly k samples represented.\n",
    "\n",
    "One way to achieve this in R is illustrated below.  \n",
    "\n",
    "First we generate some random data with two strata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv<-data.frame(variable=double(),strata=integer())\n",
    "sv[1:10,1]<-rnorm(10)\n",
    "sv[1:10,2]<-as.numeric(sv[,1]>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we split the data into two subsets, one for each stratum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv0<-sv[sv$strata==0,]$variable\n",
    "sv1<-sv[sv$strata==1,]$variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we perform SRSWR to sample three times from each subset and then combine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv0sample<-sv0[sample(1:length(sv0),3,replace=TRUE)]\n",
    "sv1sample<-sv1[sample(1:length(sv1),3,replace=TRUE)]\n",
    "svsample<-rbind(sv0sample,sv1sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve the same effect as steps 2 and 3 using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv0sample<-sv[sample(1:nrow(sv), 3, replace=TRUE,prob=(sv$strata==0)), 'variable']\n",
    "sv1sample<-sv[sample(1:nrow(sv), 3, replace=TRUE,prob=(sv$strata==1)), 'variable']\n",
    "svsample<-rbind(sv0sample,sv1sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| Apply stratified sampling with strata `gender`, with 50 samples in each stratum. <br>Using this sampling approach, repeat the steps we carried out for SRSWR to create a Stratified Sample Bootstrapped Histogram for Mean GPA by Gender. <br> Store the mean values for male and female inside \"maleStrata1\" and \"femaleStrata1\" variables respectively.  <br> Set the seed to iteration number itself for bootstrapped sampling. <br> Remember: the iteration numbers are from 1 to 1000.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "\n",
    "maleStrata1 = c()\n",
    "femaleStrata1 = c()\n",
    "for (i in 1:1000){\n",
    "    set.seed(i)\n",
    "    maleStrata1 = c(maleStrata1, mean(data[sample(1:nrow(data), 50, replace=TRUE,prob=data$gender==\"M\"), \"GPA\"]) )\n",
    "    set.seed(i)\n",
    "    femaleStrata1 = c(femaleStrata1, mean(data[sample(1:nrow(data), 50, replace=TRUE,prob=data$gender==\"F\"), \"GPA\"]) )\n",
    "}\n",
    "hist(maleStrata1, col=rgb(0,1,0,.2))\n",
    "hist(femaleStrata1, add=TRUE, col=rgb(1,0,0,.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematic sampling\n",
    "Systematic sampling is a sampling technique that is used to sample from a sequence of items. To sample $n$ items from a sequence of $N$ items, we sample every $k$-th item, where $k = N/n$, and the starting point is randomly sampled from the first $k$ items.\n",
    "\n",
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "|Consider using systematic sampling to sample 167 items from `data`. What is the value of $k$? How many different samples can you have? Store the average GPAs for these samples inside \"systematic1\" vector. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 835/167 = 5. There are 5 different samples depending on which of the first five items are chosen. The average GPAs for these samples are computed as follows.\n",
    "\n",
    "systematic1 = c()\n",
    "k2 = nrow(data)/167\n",
    "for (s in 1:k2) {\n",
    "    systematic1 = c(systematic1, mean(data[seq(s, nrow(data), k2),]$GPA))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extension\n",
    "\n",
    "We have included some extra material demonstrating how data has been generated for the k-anonymity part of this prac, and how the Python programming language could alternatively be used for the K-anonymity task.\n",
    "\n",
    "**THIS IS NOT ASSESSED**\n",
    "\n",
    "If you are interested, you can read through the notebooks in the [data_preparation](data_preparation) folder of Prac 2. Try to see if you can follow what is being done.\n",
    "\n",
    "The `KAnonymityWithPython` notebook demonstrates how you could potentially use the Python programming language instead of MySQL to perform the K-anonymity task of this practical.\n",
    "\n",
    "The `GetUQPrograms` notebook demonstrates *web-scraping*, which was used to generate the dataset used in this prac by scraping real programs from the https://www.uq.edu.au/study/browse.html?level=ugpg page.\n",
    "\n",
    "The `GenerateCSVs` notebook demonstrates how we can use Python to generate some sample data.\n",
    "These three notebooks demonstrate how you could potentially use the Python programming language instead of MySQL to perform the the K-anonymity task of this practical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This Line gets printed if there is no error, when Kernel -> Restart & Run All\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
