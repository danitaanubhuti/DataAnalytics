{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Complete the tasks given. \n",
    "* <strong>Please note that your code will be re-executed on new dataset files during marking. The new dataset will have the same \"Column Name\" as that of the current dataset - but the order of the columns can be different. Also the rows, and number of rows will change. Your code should give the correct answer for the new datasets as well. <u>So please don't hard-code the answers / row indexes / column indexes.</u> To reference a column you can use the column names instead of column index.</strong>\n",
    "\n",
    "* <strong>Make sure the R script submitted has no syntactical error, in which case a zero will be awarded for this Prac. Tutors will direct you on how to identify syntactical errors in your script.</strong>\n",
    "\n",
    "Additional Note\n",
    "* Variable names and strings are case-sensitive in R\n",
    "* Use of any packages to achieve the objective is strictly prohibited - unless explicitly mentioned in the question to do so\n",
    "* Any updates regarding the Pracs will be posted on <strong>Piazza</strong>\n",
    "* Your code will be tested on Version 4.x.x of R\n",
    "* You may find it helpful to read through the whole notebook and learn from the examples first, before solving the problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "After completing the tasks, download the notebook as an R file to your local system. This can be done by \"File > Download as > R\". Rename the downloaded R file with your \"UQ Login-id\" - for example, `s4477608.r`\n",
    "* Upload the downloaded file to jupyter in the same folder as that of this notebook, that is, inside \"Prac3\" folder.\n",
    "* Submit the downloaded file to BB.\n",
    "\n",
    "<strong> The R file uploaded to jupyter will be assessed automatically. So, it is very important to upload the R file to the correct location in jupyter, with the correct name ( as mentioned above ) - failing to which 0 will be awarded for this Prac. </strong>\n",
    "\n",
    "Make sure you \"Save & Checkpoint\" this jupyter notebook as well before the deadline. DO NOT rename this jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Data Transformation\n",
    "\n",
    "In computer programming, a *data type* of a variable determines what type of values it can contain and what operations can be performed. The following table summarises some of the common data types we will come across in R:\n",
    "\n",
    "| Data Type | Example                     | R Code                      |\n",
    "|-----------|-----------------------------|-----------------------------|\n",
    "| Logical   | TRUE, FALSE                 | `v <- TRUE`                 |\n",
    "| Numeric   | 12.3, 5, 999                | `v <- 23.5`                 |\n",
    "| Integer   | 1L, 34L, 0L                 | `v <- 2L`                   |\n",
    "| Character | \"a\", \"good\", \"TRUE\", '23.4' | `v <- \"TRUE\"`               |\n",
    "\n",
    "You can use the `class(var)` syntax to determine what object type a variable is, as demonstrated in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_var <- \"12345\" \n",
    "num_var <- 12345 \n",
    "class(char_var)\n",
    "class(num_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a dataset that contains inconsistent *data types* is a common data cleaning problem. The above example demonstrates two different ways the number 12345 could be expressed in a dataset, as a `character` or a `numeric` value. The data type of the variable determines what sort of operations can be performed on them.\n",
    "\n",
    "Datasets that are interpreted as the wrong data type, or that are *inconsistent* need to be cleaned so that the desired operations can be performed on the dataset. If a column that is supposed to contain integers contains characters, for example, we can no longer run numeric functions such as `mean()` on those values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a <- c(15, 20, 1, 10)\n",
    "b <- c(\"15\", \"1\", \"4\")\n",
    "\n",
    "# Valid! We can compute the mean of numberic values.\n",
    "print(mean(a))\n",
    "# You can't compute the mean from a set of characters by a number.\n",
    "print(mean(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, R contains built-in functions designed to convert between data-types. If we use the `as.numeric()` function, we can attempt to convert a `character` to a `numeric type`. Let's try the above example again, making sure we convert the characters to a numeric value before attempting to run the `mean` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b <- c(\"15\", \"1\", \"4\")\n",
    "mean(as.numeric(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting between data-types is a common feature of *data-cleaning* and *transformation*. Let's demonstrate by cleaning an example dataset.\n",
    "\n",
    "This dataset is a modified version of food-borne gastrointestinal illness in the US in 1940. The data has been modified from the original to include Brisbane-based addresses and some more obvious data integrity issues have been injected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(readr)\n",
    "oswego <- read_csv(\"OswegoTutorial.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step might be to explore the data and get a feel for the type of data we are working with. We can use the `head()` function to have a look at the first few rows of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(oswego)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to determine how large the dataset is, we can use the `dim()` function to determine the dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(oswego)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at the \"age\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oswego$age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice anything odd about this row? It looks like there was a data input error, and the number 7 has been inserted as the word \"seven\". Let's see how this affects our data analysis by trying to run the mean() function on the age column to determine the average age of people in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(oswego$age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we get an error, saying that the \"argument is not numeric or logical\". It looks like we can only run the mean function on a column that is *numeric* or *logical*. What data type is the age column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class(oswego$age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that R has interpreted this column as the 'character' data type, which explains why we can't run the mean function on it. Let's first replace the character(s) \"seven\" with \"7\" so that we can easily convert the whole column to a numeric data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oswego$age[oswego$age == \"seven\"] <- \"7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the above query a little bit. The `oswego$age == \"seven\"` is what is called a *conditional statement*, it matches values in `oswego$age` according to a specific condition. In this case, we match any of the rows that have the value \"seven\". We then set any of these rows to the character \"7\" instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| Convert the age column to the correct data-type (numeric). <br> Store the mean of the ages in \"meanAge1\" variable |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "oswego$age = as.numeric(oswego$age)\n",
    "meanAge1 = mean(oswego$age, na.rm=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that in our `oswego` dataset, we have the full address of each patient. Can you think of how this may be useful?\n",
    "\n",
    "One use of this locational data might be to see if outbreaks are clustered around particular suburbs/areas. In this case, being able to query the postcode directly would be useful, but currently the postcode is within the 'Address' column, which has the format:\n",
    "\n",
    "```\n",
    "38 Jones Road, South Brisbane, QLD 4101\n",
    "```\n",
    "\n",
    "One common aspect of preparing your data for use is making sure that it is in a format that is as simple to query as possible. For example, in the above case, if we wanted to find out what the most common suburb is which contained an outbreak, we would not be able to easily query suburb specifically with the `address` column as it contains a lot of superflous information.\n",
    "\n",
    "One solution might be to transform the data so each part of the address is in its own column - that way we query against a much simpler attribute such as postcode.\n",
    "\n",
    "Let's demonstrate this by splitting up the \"onsetdate\" column first as an example. You can see that the `onsetdate` is in the format:\n",
    "\n",
    "```\n",
    "19-Apr\n",
    "```\n",
    "If we wanted to query directly by month, we could separate the \"Month\" part of the address directly into its own column. Let's do that. We will use the \"tidyr\" library in R, a popular data transformation library. Run the code below and try and understand what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "library(tidyr)\n",
    "\n",
    "oswego_new <- separate(oswego, onsetdate, into = c(\"onset_day\", \"onset_month\"), sep = \"-\")\n",
    "head(oswego_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| There is a warning message for 1 row in the above conversion. Indeed, there is an issue with the \"oswego\" dataframe if you examine the content of that row. <br> Write your R code to fix the issue (data/row) in \"oswego\" dataframe, and also regerate \"oswego_new\" dataframe. <br> Note: You have to modify \"oswego\" dataframe, and then use `separate` function on modified \"oswego\" dataframe to re-generate the \"oswego_new\" dataframe as done above. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "\n",
    "oswego[24,'onsetdate'] = '18-Apr'\n",
    "oswego_new <- separate(oswego, onsetdate, into = c(\"onset_day\", \"onset_month\"), sep = \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've separated the columns, we can directly query \"month\" to see the range of months that these outbreaks occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique(oswego_new$onset_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this data is limited to April, June and also that a lot of the onset date data is missing, resulting in 'NA' values. Now let's see how we can get useful information from the location information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| Separate the \"address\" column of \"oswego\" dataframe into four new columns, `address`, `suburb_name`, `state` and `postcode` and store the result inside \"oswego_address_separate\" dataframe |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "oswego_address_separate <- separate(oswego, address, into = c(\"address\", \"suburb_name\", \"state\", \"postcode\"), sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| Which postcode has the most occurrences of illness? Remember, you will need to filter your data by only those who are ill. <br> Store the result inside \"mostIll1\" variable. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "illPost1 = table(oswego_address_separate[oswego_address_separate$ill==\"yes\",\"postcode\"])\n",
    "mostIll1 = names(illPost1[ illPost1==max(illPost1) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>Extension TASK</center>|\n",
    "| ---- |\n",
    "| List all postcodes and number of occurences using (1) R and (2) MySQL (tip: use group by query). This question will not be graded.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "aggregate(ill~postcode, \n",
    "          oswego_address_separate[oswego_address_separate[,\"ill\"]==\"yes\",] , \n",
    "          length\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| Calculate the mean age for each postcode, with `NA` excluded, and store the result inside \"meanAgePost1\" dataframe with column names (PostCode, MeanAge). <br> Is there any anomaly in the result? If yes, then store the cause of the anomaly in \"anomalyComment1\" variable (as a string). <br> If there is an anomaly, then store the postcodes that has anomaly inside \"anomaly1\" vector. <br> If there is no anomaly, then store `NA` inside \"anomaly1\" and \"anomalyComment1\" variables. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "meanAgePost1 = aggregate(age ~ postcode, oswego_address_separate, mean, na.rm=TRUE, na.action=NULL)\n",
    "colnames(meanAgePost1) = c(\"PostCode\", \"MeanAge\")\n",
    "\n",
    "anomaly1 = 4021\n",
    "\n",
    "anomalyComment1  = \"4021 has a mean age of 171.25! By inspecting the ages of people living there, we can see that one person has age 622! Clearly there is a data-entry error.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Data Exploration\n",
    "\n",
    "In Prac. 1, we first encountered the HR Analytics dataset.\n",
    "\n",
    "The question we seek to answer is: *Why are our best and most experienced employees leaving?*\n",
    "\n",
    "To get to grips with the data, we will carry out some exploratory data analysis (EDA) techniques in R.\n",
    "\n",
    "Firstly, let's import the data and look at a few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "library(readr)\n",
    "HR_comma_sep <- read_csv(\"HR_comma_sep.csv\")\n",
    "HR_comma_sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| What type of variable is `left`? Here \"type\" refers to the variable types mentioned in lecture. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]. Include the desciptive portion of the answer as an R comment.\n",
    "\n",
    "# left is a binary/boolean variable indicating whether an employee has left or not. It is a special kind of categorical variable.\n",
    "# You may confirm that left indeed has two values only by running unique(HR_comma_sep$left)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's count the number of rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(!complete.cases(HR_comma_sep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no missing data, we can proceed with our analysis on the complete data set.\n",
    "\n",
    "Let's explore some simple quantitative summaries.  The default summary statistics are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(HR_comma_sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These statistics tell us a bit about each variable in isolation; what we would like to do is obtain statistics pertinent to our question.\n",
    "\n",
    "To proceed, it is useful to classify each variable as either a *response* or a *predictor*.  For this problem and data set, it is clear that `left` is the response and all other variables are predictors.\n",
    "\n",
    "For example: What is the breakdown of `left` by job Department (`sales`)?\n",
    "\n",
    "This is easily achieved by creating a *two-way contingency* table, which counts the number of intances in each `left` by `sales` cell in a two by two table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table<-table(HR_comma_sep$sales,HR_comma_sep$left)\n",
    "test_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginal counts and proportions are easily created as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin.table(test_table) \n",
    "margin.table(test_table,1)\n",
    "margin.table(test_table,2)\n",
    "\n",
    "prop.table(test_table)\n",
    "prop.table(test_table,1)\n",
    "prop.table(test_table,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These proportions can also be conveniently visualised using a mosaic plot, where the widths of rectangles are proportional to the number of observations in the x variable categories (`Department`), and, for each x variable category, the heights are proportional to the number of observations in the corresponding y variable categories (`Left`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaicplot(test_table,main=\"Mosaic Plot of Left by Department\",xlab=\"Department\",ylab=\"Left\",las=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| Which Department has the highest proportion of employees that have left, relative to that department?  Which Department has the lowest? <br> Store the answer inside \"highest1\" and \"lowest1\" R variables respectively. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "props1 = test_table[,\"1\"] / test_table[,\"0\"]\n",
    "highest1 = names(props1[props1==max(props1)])\n",
    "lowest1 = names(props1[props1==min(props1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the breakdown of average hours worked per month by left?\n",
    "\n",
    "We will first create visual summaries using boxplots (which display summary statistics visually)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(HR_comma_sep$average_montly_hours~HR_comma_sep$left,xlab=\"Left\",ylab=\"Average Monthly Hours\",main=\"Boxplot of Average Monthly Hours by Left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple visual summary suggests that employees that left typically worked longer hours.\n",
    "\n",
    "We can get a more detailed view by constructing a histogram as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(HR_comma_sep$average_montly_hours[HR_comma_sep$left==1], col=rgb(1,0,0,0.5),main=\"Histogram for Average Monthly Hours by Left\", xlab=\"Average Monthly Hours\")\n",
    "hist(HR_comma_sep$average_montly_hours[HR_comma_sep$left==0], col=rgb(0,0,1,0.5),add=TRUE)\n",
    "legend(\"topright\", c(\"Not Left\",\"Left\"),fill=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram reveals an interesting feature of the data; the distribution of average monthly hours for employees that left is *multimodal*.  This suggests employees that leave fall into two groups: those that work normal hours and those that work long hours.\n",
    "\n",
    "The same information can be seen from a plot of the empirical cumulative distribution function (ecdf) for average monthly hours by left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E0<-ecdf(HR_comma_sep$average_montly_hours[HR_comma_sep$left==0])\n",
    "E1<-ecdf(HR_comma_sep$average_montly_hours[HR_comma_sep$left==1])\n",
    "plot(E0,col=rgb(0,0,1,0.5),verticals = TRUE, do.points = FALSE,main=\"ECDF for Average Monthly Hours by Left\", xlab=\"Average Monthly Hours\")\n",
    "plot(E1,col=rgb(1,0,0,0.5),verticals = TRUE, do.points = FALSE,add=TRUE)\n",
    "legend(\"bottomright\", c(\"Not Left\",\"Left\"),lwd=1, lty=c(1,1),col=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the time spent in the company by left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(2,1)) # Create 2 by 1 figure\n",
    "# Plot Frequencies\n",
    "hist(HR_comma_sep$time_spend_company[HR_comma_sep$left==0],breaks=c(0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5),col=rgb(0,0,1,0.5),main=\"Histogram for Time Spent at Company by Left\", xlab=\"Time Spent at Company (Years)\",freq=TRUE)\n",
    "hist(HR_comma_sep$time_spend_company[HR_comma_sep$left==1],breaks=c(0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5),col=rgb(1,0,0,0.5),add=TRUE,freq=TRUE)\n",
    "legend(\"topright\", c(\"Not Left\",\"Left\"),fill=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)))\n",
    "\n",
    "# Plot Proportions\n",
    "hist(HR_comma_sep$time_spend_company[HR_comma_sep$left==1],breaks=c(0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5),col=rgb(1,0,0,0.5),main=\"Histogram for Time Spent at Company by Left\", xlab=\"Time Spent at Company (Years)\",freq=FALSE)\n",
    "hist(HR_comma_sep$time_spend_company[HR_comma_sep$left==0],breaks=c(0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5),col=rgb(0,0,1,0.5),add=TRUE,freq=FALSE)\n",
    "legend(\"topright\", c(\"Not Left\",\"Left\"),fill=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the above plots that the densities (`Density`) are easier than the frequency counts to visually compare, although relative magnitude information is lost in the process.\n",
    "\n",
    "The shape of the distributions for time spent at the company are noticably different, with employees that have been at the company for very short of very long periods of time being less likely to leave.  Most employees that leave the company have worked there for between three and five years, inclusive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| Construct a histogram for last evaluation by left.  What is a possible explanation for any patterns you see? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]. Include the desciptive portion of the answer as an R comment.\n",
    "# Plot Density\n",
    "hist(HR_comma_sep$last_evaluation[HR_comma_sep$left==1],col=rgb(1,0,0,0.5),main=\"Histogram for Last Evaluation at Company by Left\", xlab=\"Last Evaluation\",freq=FALSE)\n",
    "hist(HR_comma_sep$last_evaluation[HR_comma_sep$left==0],col=rgb(0,0,1,0.5),add=TRUE,freq=FALSE)\n",
    "legend(\"topright\", c(\"Not Left\",\"Left\"),fill=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)))\n",
    "\n",
    "# The evaluations for people who left show a bimodal pattern: their last evaluations are either higher than average or lower than average. Those with high evaluation perhaps believed their compensation after evaluation was insignificant. Those with low evaluation possibly left for similiar reasons, as there is an implicit feeling of undervaluedness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| Construct density plots to compare the average monthly working hours, satisfaction level, and last evaluation for those who left management and HR. Are the patterns similar or different for these two departments?|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [Place your Answer here]. Include the desciptive portion of the answer as an R comment.\n",
    "\n",
    "HR_management = subset(HR_comma_sep, sales == 'management' & left == 1)\n",
    "HR_hr = subset(HR_comma_sep, sales == 'hr' & left==1)\n",
    "\n",
    "hist(HR_hr$average_montly_hours, col=rgb(1,0,0,0.5), freq=FALSE,\n",
    "     main=\"Density for Average Monthly Hours by Left\", xlab=\"Average Monthly Hours\")\n",
    "hist(HR_management$average_montly_hours, col=rgb(0,0,1,0.5), freq=FALSE, add=TRUE)\n",
    "legend(\"topright\", c(\"HR\", \"Management\"),pch=c(1,1),col=c(rgb(1,0,0,0.5), rgb(0,0,1,0.5)))\n",
    "\n",
    "hist(HR_hr$satisfaction_level, col=rgb(1,0,0,0.5), freq=FALSE,\n",
    "    main=\"Density for Satisfaction Level by Left\", xlab=\"Satisfaction\")\n",
    "hist(HR_management$satisfaction_level, col=rgb(0,0,1,0.5), freq=FALSE, add=TRUE)\n",
    "legend(\"topright\", c(\"HR\", \"Management\"),pch=c(1,1),col=c(rgb(1,0,0,0.5), rgb(0,0,1,0.5)))\n",
    "\n",
    "hist(HR_hr$number_project, col=rgb(1,0,0,0.5), freq=FALSE, \n",
    "     main=\"Density for Average Monthly Hours by Left\", xlab=\"Average Monthly Hours\")\n",
    "hist(HR_management$number_project, col=rgb(0,0,1,0.5), freq=FALSE, add=TRUE)\n",
    "legend(\"topright\", c(\"HR\", \"Management\"),pch=c(1,1),col=c(rgb(1,0,0,0.5), rgb(0,0,1,0.5)))\n",
    "\n",
    "# The density plots for the two departments are slightly different, but qualitatively similar. This suggests that being in a specific department may not be an important reason for why people leave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wish to plot last evaluation and average monthly hours by leave.  One way to do this is with a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(HR_comma_sep$average_montly_hours[HR_comma_sep$left==0],HR_comma_sep$last_evaluation[HR_comma_sep$left==0],main=\"Scatter Plot for Average Monthly Hours vs \\n Last Evalation by Left\", xlab=\"Average Monthly Hours\",ylab=\"Last Evaluation\",col=rgb(0,0,1,0.5))\n",
    "points(HR_comma_sep$average_montly_hours[HR_comma_sep$left==1],HR_comma_sep$last_evaluation[HR_comma_sep$left==1],col=rgb(1,0,0,0.5))\n",
    "legend(\"bottomright\", c(\"Not Left\",\"Left\"),pch=c(1,1),col=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plot, we gain additional insight into the relationship between last evaluation, average monthly hours, and those that left.  The bulk of those leaving that worked normal hours also received low evaluations, and most of those leaving that worked long hours received high evaluations.\n",
    "\n",
    "We can break down the scatterplot further by an additional variable with the construction of a scatter plot matrix.  For instance, we might be curious how satisfaction level, last evaluation, average monthly hours, and left relate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"lattice\") #Load the `Lattice' graphics package\n",
    "HR_subset<-subset(as.data.frame(HR_comma_sep),select=c('average_montly_hours', 'last_evaluation', 'satisfaction_level', 'left'))\n",
    "super.sym <- trellis.par.get(\"superpose.symbol\") #Get Symbol Plotting Information\n",
    "splom(~HR_subset[1:3],groups=left,data = HR_subset,varnames=c(\"Average \\nMonthly \\nHours\",\"Last \\nEvaluation\",\"Satisfaction \\nLevel\"),\n",
    "      key = list(title = \"Scatterplot Matrix\",\n",
    "                 columns = 2, \n",
    "                 points = list(pch = super.sym$pch[1:2],\n",
    "                 col = super.sym$col[1:2]),\n",
    "                 text = list(c(\"Not Left\", \"Left\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code produces a very high quality scatter plot, but has a lot of parameters to set. In practice, we typically want to get a quick and dirty (but working) plot first. This can be achieved using a one-liner `splom(~HR_subset[1:3], groups=left, data=HR_subset)`, or an even simpler one-liner `splom(~HR_subset[1:3], groups=HR_subset$left)`. Try these out. Also try this one-liner: `pairs(HR_subset[,1:3], col=as.factor(HR_subset$left))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| We have demonstrated the use of a few tools for visually exploring the relationships between two variables, including mosaic plot, boxplot, histogram, ECDF plot, scatter plots. Write code to apply these tools (if applicable) to visually explore the relationship between of the number of projects and leaving. What is a possible explanation for any patterns you see? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here].  Include the desciptive portion of the answer as an R comment.\n",
    "\n",
    "# Explore number of projects and left\n",
    "\n",
    "# Create multiple figure\n",
    "par(mfrow=c(3,2))\n",
    "\n",
    "# Plot properties\n",
    "label1 <- 'Number of Projects Completed'\n",
    "label2 <- 'Left'\n",
    "xBreaks <- c(seq(0, 7, length=8))\n",
    "red <- rgb(1,0,0,0.5)\n",
    "blue <- rgb(0,0,1,0.5)\n",
    "\n",
    "# Table for mosaic\n",
    "project_table <- table(HR_comma_sep$number_project, HR_comma_sep$left)\n",
    "\n",
    "# mosaic\n",
    "mosaicplot(project_table, main=\"Mosaic Plot of Left by Projects\", xlab=label1, ylab=label2, las=2)\n",
    "\n",
    "# boxplot\n",
    "boxplot(HR_comma_sep$number_project~HR_comma_sep$left, xlab=label2, ylab=label1, main='Boxplot of Left by Projects')\n",
    "\n",
    "# histogram - frequency\n",
    "hist(HR_comma_sep$number_project[HR_comma_sep$left==1], col=red, freq=TRUE, breaks=xBreaks, ylim=c(0,4500),\n",
    "    main='Histogram for Number of Projects by Left', cex.main=0.9, xlab=label1)\n",
    "hist(HR_comma_sep$number_project[HR_comma_sep$left==0], add=TRUE, col=blue, freq=TRUE, breaks=xBreaks, cex.main=0.9)\n",
    "legend('topright', c('Left', 'Not Left'), fill=c(red, blue))\n",
    "\n",
    "# histogram - density\n",
    "hist(HR_comma_sep$number_project[HR_comma_sep$left==1], col=red, freq=FALSE, breaks=xBreaks,\n",
    "    main='Histogram for Number of Projects by Left', cex.main=0.9, xlab=label1)\n",
    "hist(HR_comma_sep$number_project[HR_comma_sep$left==0], add=TRUE, col=blue, freq=FALSE, breaks=xBreaks, \n",
    "     cex.main=0.9)\n",
    "legend('topright', c('Left', 'Not Left'), fill=c(red, blue))\n",
    "\n",
    "# Empircal Cumulative Distribution \n",
    "E0 <- ecdf(HR_comma_sep$number_project[HR_comma_sep$left==0])\n",
    "E1 <- ecdf(HR_comma_sep$number_project[HR_comma_sep$left==1])\n",
    "plot(E0, col=blue, verticals=TRUE, do.points=FALSE, main='ECDF for Projects by Left', xlab=label1)\n",
    "plot(E1, col=red, verticals=TRUE, do.points=FALSE, add=TRUE)\n",
    "legend('bottomright', c('Not Left', 'Left'),lwd=1, lty=c(1,1), col=c(blue,red))\n",
    "\n",
    "# Let's look at projects completed by time at company\n",
    "plot(HR_comma_sep$time_spend_company[HR_comma_sep$left==0],HR_comma_sep$number_project[HR_comma_sep$left==0], \n",
    "     col=blue, main='Scatter plot for Time Spent at Company vs \\n Projects Completed', cex.main=0.8,\n",
    "    xlab='Time at Company (years)', ylab='Number of Projects Completed', ylim=c(0,8), xlim=c(0,10))\n",
    "points(HR_comma_sep$time_spend_company[HR_comma_sep$left==1],HR_comma_sep$number_project[HR_comma_sep$left==1],\n",
    "      col=red, pch=2)\n",
    "legend('topright', c('Not Left', 'Left'), pch=c(1,2), col=c(blue,red), cex=0.5)\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have been exploring the entire data set.  Let us return to the original question: *Why are our best and most experienced employees leaving?*\n",
    "\n",
    "To get to grips with this, we need to identify which subset of employees are \"best\" and \"most experienced\".\n",
    "Precisely what this means to any particular person is ambiguous.  When encountering ambiguity in the problem, the process of resolving that ambiguity involves a two-way dialogue with the *problem poser*.\n",
    "\n",
    "Broadly, one might imagine this subset to contain:\n",
    "\n",
    "- Employees with high evaluations.\n",
    "- Employees the have been with the company for a while.\n",
    "\n",
    "Additional criteria might be:\n",
    "\n",
    "- Employees that work on a large number of projects.\n",
    "- Employees that work a lot.\n",
    "\n",
    "For now, suppose that the \"best\" employees are those with an evaluation of 0.8 or higher, and the \"most experienced\" employees are those that have been with the company for 4 or more years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| Create a subset of the \"best\" and \"most experienced\" employees by appropriately filtering the entire data set. Store the result inside \"bestExp1\" dataframe. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "bestExp1 = HR_comma_sep[ HR_comma_sep$last_evaluation>=0.8 & HR_comma_sep$time_spend_company>=4 , ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| Perform EDA on the subset of \"best\" and \"most experienced\" employees you just created.  What is a possible explanation for any patterns you see? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]. Include the desciptive portion of the answer as an R comment.\n",
    "# Make scatterplot matrix\n",
    "library('lattice')\n",
    "key_columns <- subset(as.data.frame(bestExp1), select=c('satisfaction_level', 'number_project', 'average_montly_hours','time_spend_company', 'left'))\n",
    "super.sym <- trellis.par.get(\"superpose.symbol\") #Get Symbol Plotting Information\n",
    "splom(~key_columns[1:4],groups=left,data = key_columns,varnames=c(\"Satisfaction \\nLevel\", 'Number of \\nProjects', \"Average \\nMonthly \\nHours\", 'Time \\nSpent at \\nCompany'),\n",
    "      key = list(title = \"Scatterplot Matrix\",\n",
    "                 columns = 2, \n",
    "                 points = list(pch = super.sym$pch[1:2],\n",
    "                 col = super.sym$col[1:2]),\n",
    "                 text = list(c(\"Not Left\", \"Left\"))))\n",
    "\n",
    "# Left compared with promotion\n",
    "comp_table <- table(bestExp1$promotion_last_5years, bestExp1$left)\n",
    "mosaicplot(comp_table, main='Mosaic Plot of Left and if Received Promotion', xlab='Promotion Last 5 years', ylab='Left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Unusual Data\n",
    "\n",
    "Codifying which observations are \"unusual\" goes hand-in-hand with the statistical model for data; in particular the assumptions we make about the distribution of the residuals.\n",
    "\n",
    "A common assumption in statistical analyses is that the residuals follow a *normal distribution*.\n",
    "\n",
    "Let's simulate 200 normally distributed observations, synthesising heights (in cm) of adult men, plot a histogram, and display standard summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_data<-rnorm(200, mean = 174.46, sd = 7.15) \n",
    "hist(height_data,col=rgb(0,0,1,0.5),main=\"Histogram for Synthetic Height Data\", xlab=\"Height (cm)\")\n",
    "summary(height_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice the symmetric unimodal shape of the histogram.  Re-run the code above several times and observe how the distribution of the data retains these characteristics.\n",
    "\n",
    "Next, we will create a copy of the data, artificially convert the first 10 observations from units of cm to units of inches, plot another histogram, and display standard summary statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_data_out<-height_data\n",
    "height_data_out[1:10]<-height_data_out[1:10]*0.3937007874\n",
    "hist(height_data_out,col=rgb(0,0,1,0.5),main=\"Histogram for Synthetic Height Data (with Unusual Obs.)\", xlab=\"Height (cm)\")\n",
    "summary(height_data_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this artifical data, it is immediately clear from the histogram that there are unusual observations, widely separated from the bulk.\n",
    "\n",
    "Despite both being measures of central tendency, you will notice that the `mean` values have changed noticably, but the `median` values have barely moved.  This is because the `mean` is a sensitive statistic (and consequently is greatly impacted by unusual observations; influential observations in this case).  On the other hand the `median` is a rather insensitive (a *robust statistic*, and is hardly impacted by unusual observations).\n",
    "\n",
    "Using simple robust measures of central tendency (median) and variability [interquartile range (IQR)], the boxplot visually flags observations that lie outside $\\textrm{Median} \\pm 1.5 \\times \\textrm{IQR}$.  These are the observations that may depart from the underlying assumption of normality; the outliers.\n",
    "\n",
    "Let's create and store a boxplot of the height data with unusual observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdo_box<-boxplot(height_data_out,main=\"Boxplot of Synthetic Height Data (with Unusual Obs.)\", ylab=\"Height (cm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our artifical unusual observations are clearly labelled as outliers.  \n",
    "\n",
    "We can extract them from the stored boxplot as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdo_box$out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this instance, after examining the outliers and the context of the data set, it is clear that the reason for these outliers is a simple unit change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Data Imputation\n",
    "\n",
    "Let's load up Karl Pearsons' data on the heights (in inches) of fathers and their sons, produce a scatter plot of the data, and display standard summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"readr\")\n",
    "fheight<-read_csv(\"./PearsonFather.csv\",col_names=\"fheight\",col_types=\"d\")\n",
    "sheight<-read_csv(\"./PearsonSon.csv\",col_names=\"sheight\",col_types=\"d\")\n",
    "fs_height<-data.frame(fheight,sheight)\n",
    "plot(fs_height$fheight,fs_height$sheight,main=\"Father and Son Heights\", xlab=\"Father Heights (in)\",ylab=\"Son Heights (in)\",col=rgb(1,0.3,0.1,0.5))\n",
    "summary(fs_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a version of the data set with both father and son heights missing completely at random (MCAR), plot the data, and display summary statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fheight_MCAR<-fheight\n",
    "set.seed(55)\n",
    "fheight_MCAR[rbinom(nrow(fheight),1,0.1)==1,]<-NA\n",
    "sheight_MCAR<-sheight\n",
    "set.seed(55)\n",
    "sheight_MCAR[rbinom(nrow(sheight),1,0.1)==1,]<-NA\n",
    "fs_height_MCAR<-data.frame(fheight_MCAR,sheight_MCAR)\n",
    "plot(fs_height_MCAR$fheight,fs_height_MCAR$sheight,main=\"Father and Son Heights (MCAR)\", xlab=\"Father Heights (in)\",ylab=\"Son Heights (in)\",col=rgb(1,0.3,0.1,0.5))\n",
    "summary(fs_height_MCAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets examine the effect of some simple imputation strategies, using the `mice` package.\n",
    "\n",
    "First up is mean imputation (**not recommended ever in practice**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"mice\")\n",
    "imp_mean<-mice(fs_height_MCAR,method=\"mean\",m=1,maxit=1)\n",
    "fsh_mean_imp<-complete(imp_mean,1)\n",
    "fsh_mean_imp$mis<-!complete.cases(fs_height_MCAR)\n",
    "plot(fsh_mean_imp$fheight[fsh_mean_imp$mis==FALSE],fsh_mean_imp$sheight[fsh_mean_imp$mis==FALSE],main=\"Father and Son Heights (MCAR)\", xlab=\"Father Heights (in)\",ylab=\"Son Heights (in)\",col=rgb(1,0.3,0.1,0.5))\n",
    "points(fsh_mean_imp$fheight[fsh_mean_imp$mis==TRUE],fsh_mean_imp$sheight[fsh_mean_imp$mis==TRUE],col=rgb(0.1,0.7,1,0.5))\n",
    "legend(\"bottomright\", c(\"Complete Cases\",\"Imputed Cases\"),pch=c(1,1),col=c(rgb(1,0.3,0.1,0.5), rgb(0.1,0.7,1,0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distortion to the data is clearly observable in the plot above.  This distortion is reflected in its effect on the standard deviations of the variables, as can be seen as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as.data.frame(lapply(fs_height,sd)) # Source Data Standard Deviations\n",
    "as.data.frame(lapply(fs_height_MCAR,sd,na.rm=TRUE)) # Complete-case MCAR Data Standard Deviations\n",
    "as.data.frame(lapply(fsh_mean_imp[1:2],sd,na.rm=TRUE)) # Mean-imputed MCAR Data Standard Deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a small change in commands, we can perform linear regression imputation (using `method=\"norm.nob\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_linreg<-mice(fs_height_MCAR,method=\"norm.nob\",m=1)\n",
    "fsh_linreg_imp<-complete(imp_linreg,1)\n",
    "fsh_linreg_imp$mis<-!complete.cases(fs_height_MCAR)\n",
    "plot(fsh_linreg_imp$fheight[fsh_linreg_imp$mis==FALSE],fsh_linreg_imp$sheight[fsh_linreg_imp$mis==FALSE],main=\"Father and Son Heights (MCAR)\", xlab=\"Father Heights (in)\",ylab=\"Son Heights (in)\",col=rgb(1,0.3,0.1,0.5))\n",
    "points(fsh_linreg_imp$fheight[fsh_linreg_imp$mis==TRUE],fsh_linreg_imp$sheight[fsh_linreg_imp$mis==TRUE],col=rgb(0.1,0.7,1,0.5))\n",
    "legend(\"bottomright\", c(\"Complete Cases\",\"Imputed Cases\"),pch=c(1,1),col=c(rgb(1,0.3,0.1,0.5), rgb(0.1,0.7,1,0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple imputation is requested by adjusting the `m=` argument; for example `m=10` will carry out the imputation process 10 times.\n",
    "\n",
    "As a final exercise, we will fit a linear regression model to (1) the original data; (2) the complete-case data; (3) data multiply-imputated with linear regression;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_all<-with(fs_height,lm(sheight~fheight))\n",
    "\n",
    "fit_MCAR<-with(fs_height_MCAR,lm(sheight~fheight))\n",
    "\n",
    "mimp_linreg<-mice(fs_height_MCAR,method=\"norm.nob\",m=10)\n",
    "fit_linreg_mimp<-with(mimp_linreg,lm(sheight~fheight))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression fits found using each approach are summarised as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(fit_all)\n",
    "summary(fit_MCAR)\n",
    "summary(pool(fit_linreg_mimp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other imputation methods include simple random imputation from observed data (`method=\"sample\"`).\n",
    "\n",
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "|Complete \"fs_height_MCAR\" dataset using simple random imputation. Use `mice` function for the same. <br> Set \"Number of multiple imputations\" in \"mice\" function to 1. Set the `seed` argument in mice function to 55. <br> Store the completed data frame (result of `complete` function) in \"simpleRandomImp1\" variable. <br> Comment on how the data imputed using this method compares to data imputed using linear regression. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]. Include the desciptive portion of the answer as an R comment.\n",
    "\n",
    "\n",
    "# Imputation by random sampling\n",
    "imp_random<-mice(fs_height_MCAR, method='sample', m=1, seed=55)\n",
    "simpleRandomImp1<-complete(imp_random,1) # grab imputed values\n",
    "\n",
    "fsh_random = simpleRandomImp1\n",
    "fsh_random$mis<-!complete.cases(fs_height_MCAR) # create binary flag\n",
    "\n",
    "# plot colours\n",
    "orange = col=rgb(1,0.3,0.1,0.5)\n",
    "blue = rgb(0.1,0.7,1,0.5)\n",
    "\n",
    "# Plot data\n",
    "plot(fsh_random$fheight[fsh_random$mis==FALSE], fsh_random$sheight[fsh_random$mis==FALSE], col=orange,\n",
    "    main=\"Father and Son Heights (MCAR)\", xlab='Father', ylab='Son')\n",
    "points(fsh_random$fheight[fsh_random$mis==TRUE], fsh_random$sheight[fsh_random$mis==TRUE], col=blue)\n",
    "points(fsh_linreg_imp$fheight[fsh_linreg_imp$mis==TRUE],fsh_linreg_imp$sheight[fsh_linreg_imp$mis==TRUE], col='green')\n",
    "legend('bottomright', c('Complete Cases', 'Random Imputed Cases', 'Linear Regression Imputed Cases'), \n",
    "       pch=c(1,1,1), col=c(orange,blue,'green'), cex=0.7)\n",
    "\n",
    "# Data imputed using the simple random method compare similarly to the linear regression imputation. Visually it is generally hard to distinguish between the two methods. Perhaps random imputation has a higher tendancy to imputate outlier values as seen with more blue points located near the data extremes, thereby weakening the linear relationship present in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension\n",
    "\n",
    "Although the focus in this course is on the more traditional tools of EDA, *text mining* and the associated visualisation of word frequencies through the use of *word clouds* are becoming increasingly prevalent.\n",
    "\n",
    "In this extension, we will step through the basic process of reading in text data, preparing text data for text mining, determining word frequencies, and visualising the resulting frequencies in a word cloud.\n",
    "\n",
    "Let's start by loading a few packages and some text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"tm\") # Text mining library\n",
    "library(\"wordcloud\") # Wordcloud plotting library\n",
    "ulysses_raw<-readLines(\"./pg1727.txt\")\n",
    "ulysses_raw<-ulysses_raw[342:10599] # Strip out front and back matter\n",
    "ulysses<-Corpus(VectorSource(ulysses_raw)) # Convert to corpus format\n",
    "inspect(ulysses[1:30]) # Inspect first 30 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process the corpus and extract frequently used words (excluding commonly used words known as *stop words*), we need to remove punctuation, numbers, case of type, stop words, and strip out any excess whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulysses <- tm_map(ulysses, content_transformer(tolower)) # Convert corpus to lower case\n",
    "ulysses <- tm_map(ulysses, removePunctuation) # Strip common punctuation\n",
    "ulysses <- tm_map(ulysses, removeNumbers) # Strip numbers \n",
    "ulysses <- tm_map(ulysses, removeWords, stopwords(\"english\")) # Strip default stop words\n",
    "\n",
    "# Strip custom stop words\n",
    "ulysses <- tm_map(ulysses, removeWords, c(\"i\",\"and\",\"are\",\"it\",\"ii\",\"iii\",\"iv\",\"v\",\"vi\",\"vii\",\"viii\",\"ix\",\"x\",\"xi\",\"xii\",\"xiii\",\"xiv\",\"xv\",\"xvi\",\"xvii\",\"xviii\",\"xix\",\"xx\",\"xxi\",\"xxii\",\"xxiii\",\"xxiv\",\"xxv\")) \n",
    "\n",
    "ulysses <- tm_map(ulysses, stripWhitespace) # Strip excess whitespace\n",
    "\n",
    "inspect(ulysses[1:30]) # Inspect first 30 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a *term document matrix*, which counts word occurence by corpus.  Then we keep only the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulysses_tdm <- TermDocumentMatrix(ulysses) # Create a table counting word occurences by corpus\n",
    "ulysses_tdm_sparse<-removeSparseTerms(ulysses_tdm , 0.995) # Keep only words that appear more than (1-0.995)*10258 (around 51) times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the sparse term document matrix, we will create word counts as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulysses_tdms_matrix <- as.matrix(ulysses_tdm_sparse) # Convert to matrix for processing\n",
    "ulysses_tdms_freqs <- sort(rowSums(ulysses_tdms_matrix),decreasing=TRUE) # Compute frequencies and sort\n",
    "ulysses_tdms_freqs_df <- data.frame(terms = names(ulysses_tdms_freqs),freq=ulysses_tdms_freqs) # Construct data frame\n",
    "head(ulysses_tdms_freqs_df, 5) # Inspect the top 5 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will plot the most common words in a *word cloud*, in which the size of the word is proportional to its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set.seed(8888)\n",
    "wordcloud(words = ulysses_tdms_freqs_df$terms, freq = ulysses_tdms_freqs_df$freq, min.freq = 1,max.words=100, random.order=FALSE, random.color=FALSE, rot.per=0.4,colors=brewer.pal(8, \"Dark2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This Line gets printed if there is no error, when Kernel -> Restart & Run All\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
