{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Complete the tasks given. \n",
    "* <strong>Please note that your code will be re-executed on new dataset files during marking. The new dataset will have the same \"Column Name\" as that of the current dataset - but the order of the columns can be different. Also the rows, and number of rows will change. Your code should give the correct answer for the new datasets as well. <u>So please don't hard-code the answers / row indexes / column indexes.</u> To reference a column you can use the column names instead of column index.</strong>\n",
    "\n",
    "* <strong>Make sure the R script submitted has no syntactical error, in which case a zero will be awarded for this Prac. Tutors will direct you on how to identify syntactical errors in your script.</strong>\n",
    "\n",
    "Additional Note\n",
    "* Variable names and strings are case-sensitive in R\n",
    "* Use of any packages to achieve the objective is strictly prohibited - unless explicitly mentioned in the question to do so\n",
    "* Any updates regarding the Pracs will be posted on <strong>Piazza</strong>\n",
    "* Your code will be tested on Version 4.x.x of R\n",
    "* You may find it helpful to read through the whole notebook and learn from the examples first, before solving the problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "* After completing the tasks, download the notebook as an R file to your local system. This can be done by \"File > Download as > R\". \n",
    "* Combine the R script of Part-1 and Part-2 of this prac into a single script and rename the same with your \"UQ Login-id\" - for example, `s4477608.r`\n",
    "* Upload the downloaded file to jupyter in the same folder as that of this notebook, that is, inside \"Prac4\" folder.\n",
    "* Submit the combined R script file to BB.\n",
    "\n",
    "<strong> The R file uploaded to jupyter will be assessed automatically. So, it is very important to upload the R file to the correct location in jupyter, with the correct name ( as mentioned above ) - failing to which 0 will be awarded for this Prac. </strong>\n",
    "\n",
    "Make sure you \"Save & Checkpoint\" this jupyter notebook as well before the deadline. DO NOT rename this jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Introduction to Hive\n",
    "\n",
    "\n",
    "## Hive Overview\n",
    "Hive provides an interface for querying and managing large datasets in HDFS, using an SQL-like language. Hive can be used to project structure onto large datasets already residing in distributed storage. Once a structure is defined, we can use the same querying language we used against a MySQL database (SQL) to generate insights on the existing data.\n",
    "\n",
    "One of the benefits of Hive is that it abstracts the complexity of large-scale parallel processing on datasets. Users can use a familiar query language (SQL) that they may already have experience in using on smaller scale data on local machines, and Hive will abstract the same query across (potentially) thousands of machines and terabytes of data.\n",
    "\n",
    "In this prac we will look at applying structure to the HR Dataset flat file we uploaded to HDFS in prac 2 so that we can query it using SQL.\n",
    "\n",
    "## Checking the file in Hadoop\n",
    "We will first need to SSH into the data7001 node to access the Hive and Hadoop command line interface.\n",
    "\n",
    "Like in Prac 2, use the Terminal (or putty for windows) program to `SSH` into `clientnode.zones.eait.uq.edu.au`, using your sXXXXXX username as the password. Remember if you are not in UQ network, you need to access `remote.labs.eait.uq.edu.au` first and than access the clientnode.\n",
    "\n",
    "To verify if the `HR_comma_sep.csv` file is in your HDFS folder run the following command:\n",
    "```\n",
    "hadoop fs -ls\n",
    "```\n",
    "You should get the following output\n",
    "```\n",
    "Found 4 items\n",
    "drwx------   - sXXXXXXX hadoop          0 2018-08-01 12:00 .Trash\n",
    "-rw-r--r--   3 sXXXXXXX hadoop     566778 2018-08-29 03:05 HR_comma_sep.csv\n",
    "-rw-r--r--   3 hdfs     hadoop         17 2018-07-26 00:42 SECRET\n",
    "-rw-r--r--   3 sXXXXXXX hadoop         19 2018-08-08 04:14 example.txt\n",
    "```\n",
    "\n",
    "**If you *don't* see `HR_comma_sep.csv`, run the 2 commands bellow to get the file from the UQ cloud an than push it to Hadoop:**\n",
    "\n",
    "```\n",
    "wget https://stluc.manta.uqcloud.net/mdatascience/public/datasets/HumanResourceAnalytics/HR_comma_sep.csv\n",
    "hadoop fs -put HR_comma_sep.csv\n",
    "```\n",
    "\n",
    "We are now ready to start Hive.\n",
    "\n",
    "## Starting Hive\n",
    "\n",
    "Once you are in the `clientnode.zones.eait.uq.edu.au`, you can begin Hive by typing `hive` into the command line prompt. It may take a few seconds to initialise. When it's ready to receive commands, you should see a prompt like the following:\n",
    "\n",
    "```\n",
    "hive>\n",
    "```\n",
    "\n",
    "Hive allows you to think of your files in HDFS as a database, and query it in a similar way you would in MySQL. In order to create new tables, you will first need to connect to your database. Connect to your database using the following command, where the `sXXXXXX` is replaced with your student number. \n",
    "\n",
    "```\n",
    "hive> use sXXXXXXX;\n",
    "```\n",
    "\n",
    "If working, Hive should return an \"OK\" message along with how long the query took to make.\n",
    "\n",
    "Now that we are working within the correct database, the next step will be to create a Hive table definition for the HR dataset which we previously pushed into HDFS. First we will need to create the temporary table using the following syntax:\n",
    "\n",
    "```\n",
    "CREATE TABLE hr (\n",
    "    satisfaction_level float, \n",
    "    last_evaluation float, \n",
    "    number_project int,\n",
    "    average_montly_hours int,\n",
    "    time_spend_company int,\n",
    "    Work_accident int,\n",
    "    left_job int,\n",
    "    promotion_last_5years int,\n",
    "    sales string,\n",
    "    salary string \n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ',' \n",
    "LINES TERMINATED BY '\\n' ;\n",
    "\n",
    "```\n",
    "Let's dissect the command\n",
    "- `CREATE TABLE hr` This line creates a Hive table definition called hr, as per the column and datatype defined in the round braces following it\n",
    "- `ROW FORMAT DELIMITED` This line tells Hive that the underlying data which this table points to contains one row per line \n",
    "- `FIELDS TERMINATED BY ','` This line tells Hive that the columns of the underlying data which this table points to, is delimited by comma\n",
    "- `LINES TERMINATED BY '\\n'` This line tells Hive that the rows of the underlying data which this table points to, is delimited by newline character\n",
    "\n",
    "Once we've created the table, we can point this table hr to our HR Analytics dataset from HDFS using the following command, replacing `sXXXXXX` with your student number:\n",
    "\n",
    "```\n",
    "LOAD DATA INPATH '/user/sXXXXXX/HR_comma_sep.csv' OVERWRITE INTO TABLE hr;\n",
    "```\n",
    "\n",
    "\n",
    "Now that we have created a Hive table definition called `hr`, we can query the table exactly like we did in phpMyAdmin, using SQL. As an interface, you will see that Hive is very similar to a traditional database. However, instead of using a database backend, Hive can abstract our queries over many machines. Although not obvious in a dataset this size, if we had a dataset that was several terabytes in size, you would see significant performance gain over a tradition database as it is backed by a distributed file system and the queries can be distributed across several machines. \n",
    "\n",
    "For a reference in Hive commands check their official documentation:\n",
    "https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer the following questions and **include the SQL query used to determine the answer**.\n",
    "\n",
    "Please note that unlike phpMyAdmin, you will need to ensure that you end each statement with a ';'.\n",
    "\n",
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| How many entries were there in the HR dataset? <br> Store the answer and SQL query inside \"entries1\" and \"entries1_SQL\" R variables respectively. <br> You can hard-code the answer for this question. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here].\n",
    "entries1 = 14999 ;\n",
    "entries1_SQL = \"SELECT COUNT(*) FROM hr;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| What was the average number of monthly hours? <br> Store the answer and SQL query inside \"avgHrs1\" and \"avgHrs1_SQL\" R variables respectively. <br> You can hard-code the answer for this question. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here].\n",
    "avgHrs1 = 201.0503366891126\n",
    "avgHrs1_SQL = \"SELECT AVG(average_montly_hours) FROM hr;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK</center>|\n",
    "| ---- |\n",
    "| What was the average number of monthly hours by those who left their job? <br> Store the answer and SQL query inside \"avgLeft1\" and \"avgLeft1_SQL\" R variables respectively. <br> You can hard-code the answer for this question.  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here].\n",
    "avgLeft1 = 207.41921030523662\n",
    "avgLeft1_SQL = \"SELECT AVG(average_montly_hours) FROM hr WHERE left_job = 1;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike MySQL, you can see that some of your queries were submitted as *jobs*, and may have taken a few seconds to process. Jobs are a common way of describing how processing is submitted to large distributed systems. Unlike your phpMyAdmin database that was only used by you, large distributed systems are often shared and processing may take hours or months. This means that often a job has to be submitted, picked up by some sort of job management process, directed to the appropriate node/s, distribute processing across multiple jobs, maintain a job queue and collate the results across multiple nodes (among many other things!). You can see why this complexity adds time - especially since many of these interactions are over a network. The benefits of a distributed system are only apparent when your dataset is big enough to warrant using one. In our case of our small HR Dataset, it is actually much faster to use R or MySQL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This Line gets printed if there is no error, when Kernel -> Restart & Run All\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
